xlim = c(0, 1),
ylim = c(0, 3),
cex.lab = 1.5,
cex.axis = 1.5)
polygon(density(d$mean_fwc), col = "gray")
for(taxon in seq_along(func.grp1)) lines(density(d$mean_fwc[d[, func.grp1[taxon]] == 1]), col = plot.col[func])
}
?polygon
polygon(density(d$mean_fwc), col = "gray20", border = NA)
par(mfrow = c(2, 2))
for(func in seq_along(func.grp)){
func.grp1 <- func.grp[[func]]
plot(1,
type = "n",
main = func.name[func],
cex.main = 2,
xlab = "Mean fraction woody cover",
ylab = "Density",
xlim = c(0, 1),
ylim = c(0, 3),
cex.lab = 1.5,
cex.axis = 1.5)
polygon(density(d$mean_fwc), col = "gray80", border = NA)
for(taxon in seq_along(func.grp1)) lines(density(d$mean_fwc[d[, func.grp1[taxon]] == 1]), col = plot.col[func])
}
pdf("C:/Users/adu/OneDrive - Colostate/Desktop/Manuscripts/Other authors/Patterson et al_reply to Quinn & Lepre/R1/Response/Figures/Mean fwc & presences density plots_10-27-21.pdf")
par(mfrow = c(2, 2))
for(func in seq_along(func.grp)){
func.grp1 <- func.grp[[func]]
plot(1,
type = "n",
main = func.name[func],
cex.main = 2,
xlab = "Mean fraction woody cover",
ylab = "Density",
xlim = c(0, 1),
ylim = c(0, 3),
cex.lab = 1.5,
cex.axis = 1.5)
polygon(density(d$mean_fwc), col = "gray80", border = NA)
for(taxon in seq_along(func.grp1)) lines(density(d$mean_fwc[d[, func.grp1[taxon]] == 1]), col = plot.col[func])
}
dev.off()
1709+53+75
1-(1-.05)^20
1-(1-.05/20)^20
library(VGAM)
?pbetageom
args(pnorm)
pnorm(1.96)
plot(0:20, 2,2, type="o")
plot(dbetageom(0:20, 2,2, type="o")
plot(dbetageom(0:20, 2, 2), type="o")
windows()
plot(dbetageom(0:20, 2, 2), type="o")
plot(pbetageom(0:20, 2, 2), type="o")
pbetageom(0:20, 2, 2)
plot(pbetageom(0:20, 1, 9), type="o")
pbetageom(0:20, 1, 9)
pbetageom(0:50, 1, 9)
pbetageom(0:100, 1, 9)
pbetageom(0:1000, 1, 9)
which(round(pbetageom(0:1000, 1, 9),2)==.95)[1]
pbetageom(155, 1, 9)
?floor
floor(0.44)
which(round(pbetageom(0:1000, 1, 9),3)==.95)[1]
pbetageom(170, 1, 9)
pbetageom(169, 1, 9)
pbetageom(171, 1, 9)
pbetageom(170, 1, 9)
which(round(pbetageom(0:1000, 100, 900),3)==.95)[1]
which(round(pbetageom(0:100, 100, 900),3)==.95)[1]
pbetageom(0:100, 100, 900)
10/9000
pgeom(1:100, 10/(9000+10))
plot(pgeom(1:100, 10/(9000+10)), pbetageom(1:100, 10, 9000))
abline(0,1)
plot(pgeom(1:100, 1/10), pbetageom(1:100, 1, 9))
abline(0,1)
which(round(pbetageom(0:1000, 100, 900),3)==.95)[1]
which(round(pbetageom(0:1000, 1, 9),3)==.95)[1]
pbetageom(0:1000, 1, 9)[170]
which(pbetageom(0:1000, 1, 9)>=0.95)[1]
pbetageom(0:1000, 1, 9)[171]
which(pbetageom(0:10000, 1, 9)>=0.95)[1]
pbetageom(100, 1, 9)
plot(pbetageom(100, 1, 2))
plot(pbetageom(1:100, 1, 2))
hist(rnorm(100) / rnorm(100, mean = 2, sd = 5))
hist(rnorm(100, mean = 10) / rnorm(100, mean = 2, sd = 5))
hist(log(rnorm(100, mean = 10) / rnorm(100, mean = 2, sd = 5)))
hist(log(rnorm(100, mean = 10) / rnorm(100, mean = 20, sd = 5)))
hist(log(rnorm(100, mean = 10) / rnorm(100, mean = 20, sd = 5)))
hist(log(rnorm(100, mean = 10) / rnorm(100, mean = 20, sd = 5)))
hist(log(rnorm(100, mean = 10) / rnorm(100, mean = 20, sd = 5)))
x <- rnorm(50)
y <- 2 * x + rnorm(50)
plot(x,y)
cor(x,y)
x <- rnorm(50, sd = 2)
y <- 2 * x + rnorm(50, sd = 2)
cor(x,y)
cor.test(x,y)
x <- rnorm(50, mean = 0, sd = 1)
y <- 2 * x + rnorm(50, mean = 0, sd = 1)
plot(x,y)
cor(x,y)
ccf(x,y)
test=ccf(x,y)
test$acf
small.error <- large.error <- array(dim=c(1000, 3))
n.iter <- 1000
small.error <- large.error <- array(dim=c(n.iter, 3))
ts.length <- 15
x.small <- cumsum(rnorm(ts.length, mean = 0, sd = small.sd))
y.small <- cumsum(rnorm(ts.length, mean = 0, sd = small.sd))
n.iter <- 1000
ts.length <- 15
small.sd <- 1
large.sd <- 5
small.error <- large.error <- array(dim=c(n.iter, 3))
x.small <- cumsum(rnorm(ts.length, mean = 0, sd = small.sd))
y.small <- cumsum(rnorm(ts.length, mean = 0, sd = small.sd))
plot(x.small, type="l")
plot(x.large, type="l")
plot(y.small, type="l")
plot(x.small, y.small)
test=cor.test(x.small, y.small)
test$statistic
cor(x.small, y.small)
test$parameter
test$estimate
test$conf.int
length(test$conf.int)
n.iter <- 1000
ts.length <- 15
small.sd <- 1
large.sd <- 5
small.error <- large.error <- array(dim=c(n.iter, 3))
for(i in seq_len(nrow(small.error))){
x.small <- cumsum(rnorm(ts.length, mean = 0, sd = small.sd))
y.small <- cumsum(rnorm(ts.length, mean = 0, sd = small.sd))
x.large <- cumsum(rnorm(ts.length, mean = 0, sd = large.sd))
y.large <- cumsum(rnorm(ts.length, mean = 0, sd = large.sd))
cor.res.small <- cor.test(x.small, y.small)
cor.res.large <- cor.test(x.large, y.large)
small.error[i, 1] <- cor.res.small$estimate
small.error[i, 2:3] <- cor.res.small$conf.int
large.error[i, 1] <- cor.res.large$estimate
large.error[i, 2:3] <- cor.res.large$conf.int
}
hist(small.error[,1])
mean(small.error)
mean(large.error)
hist(small.error[, 3] - small.error[,2])
mean(small.error[, 3] - small.error[,2])
mean(large.error[, 3] - large.error[,2])
hist(large.error[, 3] - large.error[,2])
ts.length <- 15
t
T
n.iter <- 1000
small.res <- large.res <- array(n.iter, 3)
cor.res.small$estimate
n.iter <- 1000
ts.length <- 15
small.sd <- 1
big.sd <- 5
small.res <- large.res <- array(n.iter, 3)
small.res <- large.res <- array(dim=c(n.iter, 3))
dim(small.res)
for(i in seq_len(n.iter)){
x.small <- rnorm(ts.length, sd = small.sd)
y.small <- rnorm(ts.length, sd = small.sd)
x.big <- rnorm(ts.length, sd = big.sd)
y.big <- rnorm(ts.length, sd = big.sd)
cor.res.small <- cor.test(x.small, y.small)
cor.res.large <- cor.test(x.big, y.big)
small.res[i, 1] <- cor.res.small$estimate
small.res[i, 2:3] <- cor.res.small$conf.int
large.res[i, 1] <- cor.res.large$estimate
large.res[i, 2:3] <- cor.res.large$conf.int
}
hist(small.res[,1])
hist(large.res[,1])
mean(large.res[,1])
mean(small.res[,1])
sd(small.res[,1])
sd(large.res[,1])
n.iter <- 1000
ts.length <- 15
small.sd <- 1
big.sd <- 5
small.res <- large.res <- array(dim=c(n.iter, 3))
for(i in seq_len(n.iter)){
x.small <- rnorm(ts.length, sd = small.sd)
y.small <- rnorm(ts.length, sd = small.sd) * 2 + rnorm(ts.length)
x.big <- rnorm(ts.length, sd = big.sd)
y.big <- rnorm(ts.length, sd = big.sd) * 2 + rnorm(ts.length)
cor.res.small <- cor.test(x.small, y.small)
cor.res.large <- cor.test(x.big, y.big)
small.res[i, 1] <- cor.res.small$estimate
small.res[i, 2:3] <- cor.res.small$conf.int
large.res[i, 1] <- cor.res.large$estimate
large.res[i, 2:3] <- cor.res.large$conf.int
}
hist(small.res[,1])
n.iter <- 1000
ts.length <- 15
small.sd <- 1
big.sd <- 5
small.res <- large.res <- array(dim=c(n.iter, 3))
for(i in seq_len(n.iter)){
x.small <- rnorm(ts.length, sd = small.sd)
y.small <- x.small * 2 + rnorm(ts.length)
x.big <- rnorm(ts.length, sd = big.sd)
y.big <- x.big * 2 + rnorm(ts.length)
cor.res.small <- cor.test(x.small, y.small)
cor.res.large <- cor.test(x.big, y.big)
small.res[i, 1] <- cor.res.small$estimate
small.res[i, 2:3] <- cor.res.small$conf.int
large.res[i, 1] <- cor.res.large$estimate
large.res[i, 2:3] <- cor.res.large$conf.int
}
hist
hist(small.res[,1])
mean(small.res[,1])
mean(large.res[,1])
hist(large.res[,1])
n.iter <- 1000
ts.length <- 15
small.sd <- 1
big.sd <- 5
small.res <- large.res <- array(dim=c(n.iter, 3))
for(i in seq_len(n.iter)){
x <- runif(ts.length)
y.small <- x * 2 + rnorm(ts.length, sd = small.sd)
y.big <- x * 2 + rnorm(ts.length, sd = big.sd)
cor.res.small <- cor.test(x, y.small)
cor.res.large <- cor.test(x, y.big)
small.res[i, 1] <- cor.res.small$estimate
small.res[i, 2:3] <- cor.res.small$conf.int
large.res[i, 1] <- cor.res.large$estimate
large.res[i, 2:3] <- cor.res.large$conf.int
}
hist(small.res[,1])
hist(small.res[,2])
mean(small.res[,1])
mean(big.res[,1])
mean(large.res[,1])
hist(small.res[,1])
hist(large.res[,1])
mean(small.res[,3]-small.res[,2])
hist(small.res[,3]-small.res[,2])
hist(large.res[,3]-large.res[,2])
mean(large.res[,3]-large.res[,2])
n.iter <- 1000
ts.short <- 15
ts.long <- 50
small.res <- large.res <- array(dim=c(n.iter, 3))
for(i in seq_len(n.iter)){
x.short <- runif(ts.short)
x.long <- runif(ts.long)
y.small <- x.short * 2 + rnorm(ts.short, sd = small.sd)
y.big <- x.long * 2 + rnorm(ts.long, sd = small.sd)
cor.res.small <- cor.test(x.short, y.small)
cor.res.large <- cor.test(x.long, y.big)
small.res[i, 1] <- cor.res.small$estimate
small.res[i, 2:3] <- cor.res.small$conf.int
large.res[i, 1] <- cor.res.large$estimate
large.res[i, 2:3] <- cor.res.large$conf.int
}
windows()
hist(cor.res.small[,1])
hist(small.res[,1])
mean(small.res[,1])
windows()
hist(large.res[,1])
mean(large.res[,1])
sd(small.res)
sd(small.res[,1])
sd(large.res[,1])
test=rnorm(5)
quantile(test, c(0.025, 0.0975))
quantile(test, c(0.025, 0.975))
hist(quantile)
?quantile
sort(test)
install.packages("HDInterval", dep=T)
library(HDInterval)
?hdi
hdi(test)
quantile(test, c(0.025, 0.975))
x <- rlnorm(20)
hist(x)
hist(scale(x))
y <- runif(20)
cor(x,y)
cor(scale(x),y)
IQR(x)
IQR(scale(x))
## Read in CMR results
CMR.res <- readRDS("C:/Users/adu/OneDrive - Colostate/Desktop/Manuscripts/Other authors/Cohen et al_climate variability/GitHub/CMR files/CMR results.rds")
turk_CMR.res <- readRDS("C:/Users/adu/OneDrive - Colostate/Desktop/Manuscripts/Other authors/Cohen et al_climate variability/Submitted PNAS/R1/CMR files/Turkana CMR results.rds") # Turkana only
## Read in climate variability data
clim.var <- read.csv(file = "C:/Users/adu/OneDrive - Colostate/Desktop/Manuscripts/Other authors/Cohen et al_climate variability/Submitted PNAS/R1/Original Datasets/climate variability 250ka bins.csv", header = TRUE, row.names = 1)
## remove time bins from climate variability that aren't in CMR results
clim.var1 <- clim.var[seq(1, which(rownames(clim.var) == "3625")), ]
clim.var1 <- clim.var1[, -ncol(clim.var1)]
d13C <- clim.var$Turkana_psol_d13C # Turkana psol data only
d13C <- d13C[rownames(clim.var) %in% seq(1125, 4125, 250)] # get out data sychronous with mammal data
# get out origination and extinction estimates for lumped cf treatment
cmr_cf.lump <- CMR.res$neo_cf.lump # all localities cf lumped
cf.lump_orig <- rev(1 - cmr_cf.lump$estimate[grep("Gamma", rownames(cmr_cf.lump))]) # all localities origination
cf.lump_extinct <- rev(1 - cmr_cf.lump$estimate[grep("Phi", rownames(cmr_cf.lump))]) # all localities extinction
turk_cmr_lump <- turk_CMR.res$neo_cf.lump # turkana cf lumped
turk_lump_orig <- rev(1 - turk_cmr_lump$estimate[grep("Gamma", rownames(turk_cmr_lump))]) # turkana origination
turk_lump_extinct <- rev(1 - turk_cmr_lump$estimate[grep("Phi", rownames(turk_cmr_lump))]) # turkana extinction
cf.lump_orig
cf.lump_extinct
turk_lump_orig
turk_lump_extinct
clim.var1
dim(clim.var1)
p <- 0.01
log(8)
log(2)+log(4)
log(.05)/log(.99)
log(0.05)/(-.1)
log(0.05)/log(1-0.01)
log(0.05)/(-0.01)
log(0.05) / log(1 - p)
log(5)/(-.01)
log(0.05)
log(0.05)/(-.01)
log(0.05) / log(1 - p) # 299
?rexp
qexp(.95, rate=0.01)
qexp(.05, rate=0.01)
learnr::run_tutorial("Week1", package = "ANTH674")
install.packages("learnr", dep=T)
install.packages("devtools", dep = TRUE)
learnr::run_tutorial("Week1", package = "ANTH674")
library(learnr)
library(devtools)
devtools::install_github("andrewdu1/ANTH674")
learnr::run_tutorial("Week1", package = "ANTH674")
?acf
lh
plot(lh)
length(lh)
acf(lh)
acf(lh, lag.max=length(lh))
## Read in CMR results
CMR.res <- readRDS("CMR files/CMR results.rds")
turk_CMR.res <- readRDS("CMR files/Turkana CMR results.rds") # Turkana only
# get out origination and extinction estimates for lumped cf treatment & combine into one list
cmr_cf.lump <- CMR.res$neo_cf.lump # all localities cf lumped
turk_cmr_lump <- turk_CMR.res$neo_cf.lump # turkana cf lumped
cmr.rates <- list(
cf.lump_orig = 1 - cmr_cf.lump$estimate[grep("Gamma", rownames(cmr_cf.lump))], # all localities origination
cf.lump_extinct = 1 - cmr_cf.lump$estimate[grep("Phi", rownames(cmr_cf.lump))], # all localities extinction
turk_lump_orig = 1 - turk_cmr_lump$estimate[grep("Gamma", rownames(turk_cmr_lump))], # turkana origination
turk_lump_extinct = 1 - turk_cmr_lump$estimate[grep("Phi", rownames(turk_cmr_lump))] # turkana extinction
)
# create a list of ages, corresponding to rates in cmr.rates
bins <- seq(3.75, 0, -0.25) # create time bins of 0.25 Myr from 3.75-0 Ma
time.bin.mid <- bins[-length(bins)] + diff(bins)[1] / 2 # bin age midpoints
turk.bin.mid <- seq(4.125, 1.125, -0.25) # time bin midpoints for 0.25 Myr from 4.125-1.125 for Turkana
ages <- list(
cf.lump_orig_ages = time.bin.mid[-1],
cf.lump_extinct_ages = time.bin.mid[-length(time.bin.mid)],
turk_lump_orig_ages = turk.bin.mid[-1],
turk_lump_extinct_ages = turk.bin.mid[-length(turk.bin.mid)]
)
## Read in climate variability data
clim.var <- read.csv(file = "original datasets/climate variability 250ka bins.csv", header = TRUE, row.names = 1)
## remove time bins from climate variability that aren't in CMR results
clim.var1 <- clim.var[seq(1, which(rownames(clim.var) == "3625")), ]
clim.var1 <- clim.var1[, -ncol(clim.var1)]
clim.var1 <- clim.var1[seq(nrow(clim.var1), 1), ] # reverse row order, so time bins match those of CMR rates
d13C <- clim.var$Turkana_psol_d13C # Turkana psol data only
d13C <- d13C[rownames(clim.var) %in% seq(1125, 4125, 250)] # get out data synchronous with mammal data
d13C <- rev(d13C) # reverse row order, so time bins match those of CMR rates
####################################################
## Detrend each time series using a LOWESS regression
# write a function to do this
# ARGUMENTS:
# age: age associated with each data point in the time series
# x: variable value associated with each data point
# span: smoothing span of LOWESS regression (passed on to lowess() function)
detrend <- function(age, x, span = 2 / 3){
lowess.res <- lowess(age, x, f = span)
return(x - rev(lowess.res$y)) # rev() is needed to maintain order from older to younger
}
# climate variables
clim.var1.detrend <- apply(clim.var1, 2, function(clim) detrend(age = as.numeric(rownames(clim.var1)) / 1000, x = clim))
# d13C paleosol
d13C.detrend <- detrend(age = seq(4.125, 1.125, -0.25), x = d13C)
# CMR rates
cmr.rates.detrend <- mapply(detrend, age = ages, x = cmr.rates)
## run CCF analyses
# create function for estimate CCF and its p-value
# ARGUMENTS:
# climate: climate time series
# cmr_rate: CMR rate time series
# ccf.lag: max #lags to consider in CCF
# acf.lag: max. # lags in ACF for computing SE
CCF <- function(climate, cmr_rate, ccf.lag, acf.lag){
n <- length(climate)
ccf.res <- ccf(cmr_rate, climate, plot = FALSE)[0:ccf.lag]
ccf.hat <- ccf.res$acf
acf.clim <- acf(climate, lag.max = acf.lag, plot = FALSE)$acf
acf.cmr <- acf(cmr_rate, lag.max = acf.lag, plot = FALSE)$acf
ccf.se <- sqrt((1 + 2 * sum(acf.clim * acf.cmr)) / n)
p.vals <- sapply(ccf.hat, function(x) pnorm(abs(x), sd = ccf.se, lower.tail = FALSE) * 2)
return(list(lag.n = seq(0, ccf.lag), ccf.hat = ccf.hat, ccf.se = ccf.se, p.vals = p.vals))
}
# run CCFs for E. African data
ccf.res.orig <- apply(clim.var1.detrend, 2, function(clim) CCF(climate = clim[-1], cmr_rate = cmr.rates.detrend$cf.lump_orig, ccf.lag = 2, acf.lag = length(clim) - 1))
ccf.res.extinct <- apply(clim.var1.detrend, 2, function(clim) CCF(climate = clim[-length(clim)], cmr_rate = cmr.rates.detrend$cf.lump_extinct, ccf.lag = 2, acf.lag = length(clim) - 1))
# run CCFs for Turkana data
ccf.turk.orig <- CCF(climate = d13C.detrend[-1], cmr_rate = cmr.rates.detrend$turk_lump_orig, ccf.lag = 2, acf.lag = length(d13C.detrend) - 1)
ccf.turk.extinct <- CCF(climate = d13C.detrend[-length(d13C)], cmr_rate = cmr.rates.detrend$turk_lump_extinct, ccf.lag = 2, acf.lag = length(d13C.detrend) - 1)
## see how many P-values are significant
p.vals <- c(sapply(ccf.res.orig, function(x) x$p.vals),
sapply(ccf.res.extinct, function(x) x$p.vals),
ccf.turk.orig$p.vals,
ccf.turk.extinct$p.vals)
sum(p.vals <= 0.05) # none are significant
setwd("C:/Users/adu/OneDrive - Colostate/Desktop/Manuscripts/Other authors/Cohen et al_climate variability/GitHub/")
## Read in CMR results
CMR.res <- readRDS("CMR files/CMR results.rds")
turk_CMR.res <- readRDS("CMR files/Turkana CMR results.rds") # Turkana only
# get out origination and extinction estimates for lumped cf treatment & combine into one list
cmr_cf.lump <- CMR.res$neo_cf.lump # all localities cf lumped
turk_cmr_lump <- turk_CMR.res$neo_cf.lump # turkana cf lumped
cmr.rates <- list(
cf.lump_orig = 1 - cmr_cf.lump$estimate[grep("Gamma", rownames(cmr_cf.lump))], # all localities origination
cf.lump_extinct = 1 - cmr_cf.lump$estimate[grep("Phi", rownames(cmr_cf.lump))], # all localities extinction
turk_lump_orig = 1 - turk_cmr_lump$estimate[grep("Gamma", rownames(turk_cmr_lump))], # turkana origination
turk_lump_extinct = 1 - turk_cmr_lump$estimate[grep("Phi", rownames(turk_cmr_lump))] # turkana extinction
)
# create a list of ages, corresponding to rates in cmr.rates
bins <- seq(3.75, 0, -0.25) # create time bins of 0.25 Myr from 3.75-0 Ma
time.bin.mid <- bins[-length(bins)] + diff(bins)[1] / 2 # bin age midpoints
turk.bin.mid <- seq(4.125, 1.125, -0.25) # time bin midpoints for 0.25 Myr from 4.125-1.125 for Turkana
ages <- list(
cf.lump_orig_ages = time.bin.mid[-1],
cf.lump_extinct_ages = time.bin.mid[-length(time.bin.mid)],
turk_lump_orig_ages = turk.bin.mid[-1],
turk_lump_extinct_ages = turk.bin.mid[-length(turk.bin.mid)]
)
## Read in climate variability data
clim.var <- read.csv(file = "original datasets/climate variability 250ka bins.csv", header = TRUE, row.names = 1)
## remove time bins from climate variability that aren't in CMR results
clim.var1 <- clim.var[seq(1, which(rownames(clim.var) == "3625")), ]
clim.var1 <- clim.var1[, -ncol(clim.var1)]
clim.var1 <- clim.var1[seq(nrow(clim.var1), 1), ] # reverse row order, so time bins match those of CMR rates
d13C <- clim.var$Turkana_psol_d13C # Turkana psol data only
d13C <- d13C[rownames(clim.var) %in% seq(1125, 4125, 250)] # get out data synchronous with mammal data
d13C <- rev(d13C) # reverse row order, so time bins match those of CMR rates
####################################################
## Detrend each time series using a LOWESS regression
# write a function to do this
# ARGUMENTS:
# age: age associated with each data point in the time series
# x: variable value associated with each data point
# span: smoothing span of LOWESS regression (passed on to lowess() function)
detrend <- function(age, x, span = 2 / 3){
lowess.res <- lowess(age, x, f = span)
return(x - rev(lowess.res$y)) # rev() is needed to maintain order from older to younger
}
# climate variables
clim.var1.detrend <- apply(clim.var1, 2, function(clim) detrend(age = as.numeric(rownames(clim.var1)) / 1000, x = clim))
# d13C paleosol
d13C.detrend <- detrend(age = seq(4.125, 1.125, -0.25), x = d13C)
# CMR rates
cmr.rates.detrend <- mapply(detrend, age = ages, x = cmr.rates)
## run CCF analyses
# create function for estimate CCF and its p-value
# ARGUMENTS:
# climate: climate time series
# cmr_rate: CMR rate time series
# ccf.lag: max #lags to consider in CCF
# acf.lag: max. # lags in ACF for computing SE
CCF <- function(climate, cmr_rate, ccf.lag, acf.lag){
n <- length(climate)
ccf.res <- ccf(cmr_rate, climate, plot = FALSE)[0:ccf.lag]
ccf.hat <- ccf.res$acf
acf.clim <- acf(climate, lag.max = acf.lag, plot = FALSE)$acf
acf.cmr <- acf(cmr_rate, lag.max = acf.lag, plot = FALSE)$acf
ccf.se <- sqrt((1 + 2 * sum(acf.clim * acf.cmr)) / n)
p.vals <- sapply(ccf.hat, function(x) pnorm(abs(x), sd = ccf.se, lower.tail = FALSE) * 2)
return(list(lag.n = seq(0, ccf.lag), ccf.hat = ccf.hat, ccf.se = ccf.se, p.vals = p.vals))
}
# run CCFs for E. African data
ccf.res.orig <- apply(clim.var1.detrend, 2, function(clim) CCF(climate = clim[-1], cmr_rate = cmr.rates.detrend$cf.lump_orig, ccf.lag = 2, acf.lag = length(clim) - 1))
ccf.res.extinct <- apply(clim.var1.detrend, 2, function(clim) CCF(climate = clim[-length(clim)], cmr_rate = cmr.rates.detrend$cf.lump_extinct, ccf.lag = 2, acf.lag = length(clim) - 1))
# run CCFs for Turkana data
ccf.turk.orig <- CCF(climate = d13C.detrend[-1], cmr_rate = cmr.rates.detrend$turk_lump_orig, ccf.lag = 2, acf.lag = length(d13C.detrend) - 1)
ccf.turk.extinct <- CCF(climate = d13C.detrend[-length(d13C)], cmr_rate = cmr.rates.detrend$turk_lump_extinct, ccf.lag = 2, acf.lag = length(d13C.detrend) - 1)
## see how many P-values are significant
p.vals <- c(sapply(ccf.res.orig, function(x) x$p.vals),
sapply(ccf.res.extinct, function(x) x$p.vals),
ccf.turk.orig$p.vals,
ccf.turk.extinct$p.vals)
sum(p.vals <= 0.05) # none are significant
acf(cmr.rates$cf.lump_orig)
acf(cmr.rates.detrend$cf.lump_orig_ages)
acf(clim.var1.detrend[,1])
